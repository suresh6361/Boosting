{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAUS1ETmhZvM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 1. What is Boosting in Machine Learning?\n",
        "Boosting is an ensemble technique that combines multiple weak learners (usually decision trees) to form a strong learner. It works sequentially, where each new model focuses on correcting the errors made by the previous ones.\n",
        "\n",
        "2. How does Boosting differ from Bagging?\n",
        "Aspect\tBagging\tBoosting\n",
        "Model Type\tParallel\tSequential\n",
        "Goal\tReduce variance\tReduce bias (and variance)\n",
        "Sample Usage\tBootstrapped samples\tFocus on errors of prior models\n",
        "Examples\tRandom Forest\tAdaBoost, Gradient Boosting\n",
        "\n",
        "3. What is the key idea behind AdaBoost?\n",
        "AdaBoost (Adaptive Boosting) improves model performance by:\n",
        "\n",
        "Assigning weights to training instances.\n",
        "\n",
        "Increasing weights on misclassified points.\n",
        "\n",
        "Building subsequent weak learners focused on hard-to-classify points.\n",
        "\n",
        "Combining learners with weighted voting.\n",
        "\n",
        " 4. Explain the working of AdaBoost with an example\n",
        "Example:\n",
        "Suppose you want to classify emails as spam/not spam.\n",
        "\n",
        "Initialize weights equally to all training samples.\n",
        "\n",
        "Train the first weak classifier (e.g., a decision stump).\n",
        "\n",
        "Evaluate errors:\n",
        "\n",
        "Misclassified samples → Increase their weights.\n",
        "\n",
        "Correctly classified → Decrease weights.\n",
        "\n",
        "Train the next model with the updated weights.\n",
        "\n",
        "Repeat steps 2–4.\n",
        "\n",
        "Final prediction: weighted majority vote of all weak learners.\n",
        "\n",
        "5. What is Gradient Boosting, and how is it different from AdaBoost?\n",
        "Gradient Boosting:\n",
        "\n",
        "Uses gradients (first-order derivatives) of a loss function to minimize prediction error.\n",
        "\n",
        "Fits new models on residuals (errors) of previous models.\n",
        "\n",
        "Difference:\n",
        "AdaBoost adjusts sample weights.\n",
        "\n",
        "Gradient Boosting fits new learners to residual errors.\n",
        "\n",
        "Gradient Boosting is more flexible in choosing the loss function.\n",
        "\n",
        "6. What is the loss function in Gradient Boosting?\n",
        "It depends on the task:\n",
        "\n",
        "Task\tLoss Function\n",
        "Regression\tMean Squared Error (MSE)\n",
        "Classification\tLog Loss (binary cross-entropy)\n",
        "\n",
        "7. How does XGBoost improve over traditional Gradient Boosting?\n",
        "XGBoost (Extreme Gradient Boosting) improvements:\n",
        "\n",
        "Regularization to prevent overfitting.\n",
        "\n",
        "Parallel processing for speed.\n",
        "\n",
        "Tree pruning and approximate splits for efficiency.\n",
        "\n",
        "Handling missing values natively.\n",
        "\n",
        "Cache-aware access patterns for faster computation.\n",
        "\n",
        " 8. What is the difference between XGBoost and CatBoost?\n",
        "Feature\tXGBoost\tCatBoost\n",
        "Categorical Data\tNeeds encoding (e.g., one-hot)\tHandles natively\n",
        "Training Speed\tFast\tCompetitive, optimized for GPU\n",
        "Default Settings\tRequires tuning\tBetter out-of-the-box performance\n",
        "Gradient Type\tFirst-order\tSymmetric tree with advanced gradient handling\n",
        "\n",
        " 9. What are some real-world applications of Boosting techniques?\n",
        "Fraud detection (finance)\n",
        "\n",
        "Click-through rate prediction (ads)\n",
        "\n",
        "Credit scoring (banking)\n",
        "\n",
        "Customer churn prediction\n",
        "\n",
        "Medical diagnosis\n",
        "\n",
        "Search ranking and recommendation engines\n",
        "\n",
        "10. How does regularization help in XGBoost?\n",
        "Regularization prevents overfitting by:\n",
        "\n",
        "Penalizing complex trees (L1/L2 regularization).\n",
        "\n",
        "Controlling tree depth, leaf scores, and number of leaves.\n",
        "\n",
        "Reducing variance while maintaining bias.\n",
        "\n",
        "11. What are some hyperparameters to tune in Gradient Boosting models?\n",
        "learning_rate – Shrinks contribution of each tree.\n",
        "\n",
        "n_estimators – Number of boosting rounds.\n",
        "\n",
        "max_depth – Depth of each tree.\n",
        "\n",
        "subsample – Fraction of samples used per tree.\n",
        "\n",
        "colsample_bytree – Fraction of features per tree.\n",
        "\n",
        "min_child_weight – Minimum sum of instance weight in a child.\n",
        "\n",
        "gamma – Minimum loss reduction to make a split.\n",
        "\n",
        "lambda, alpha – L2 and L1 regularization.\n",
        "\n",
        " 12. What is the concept of Feature Importance in Boosting?\n",
        "Feature importance measures how valuable each feature is in making predictions. In boosting:\n",
        "\n",
        "It shows how often a feature is used for splitting.\n",
        "\n",
        "XGBoost and CatBoost provide:\n",
        "\n",
        "Gain: Total reduction in loss due to the feature.\n",
        "\n",
        "Cover: Number of samples affected.\n",
        "\n",
        "Frequency: Number of times a feature appears in trees.\n",
        "\n",
        "13. Why is CatBoost efficient for categorical data?\n",
        "CatBoost is designed to natively handle categorical variables:\n",
        "\n",
        "Uses ordered boosting to avoid target leakage.\n",
        "\n",
        "Converts categories using target statistics with permutations.\n",
        "\n",
        "Requires minimal preprocessing—no need for one-hot or label encoding.\n"
      ],
      "metadata": {
        "id": "nxJeKK48hbYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q14 Train an AdaBoost Classifier on a sample dataset and print model accuracy\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train AdaBoost classifier\n",
        "model = AdaBoostClassifier(n_estimators=50, learning_rate=1.0, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"AdaBoost Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "n_estimators: Number of weak learners (default 50).\n",
        "\n",
        "learning_rate: Weight applied to each classifier (default 1.0).\n",
        "\n",
        "accuracy_score: Compares true vs predicted labels.\n",
        "\n",
        "\n",
        "#Q15 Train an AdaBoost Regressor and evaluate performance using Mean Absolute Error (MAE)4\n",
        "\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train AdaBoost Regressor\n",
        "model = AdaBoostRegressor(n_estimators=100, learning_rate=0.5, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "print(f\"AdaBoost Regressor MAE: {mae:.4f}\")\n",
        "\n",
        "#AdaBoostRegressor: Uses a series of weak regressors (default: decision stumps).\n",
        "\n",
        "#n_estimators: Number of weak learners.\n",
        "\n",
        "#learning_rate: Step size shrinkage to prevent overfitting.\n",
        "\n",
        "#mean_absolute_error: Measures average absolute prediction error.\n",
        "\n",
        "#Q16-Train a Gradient Boosting Classifier on the Breast Cancer dataset and print feature importance4\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Gradient Boosting Classifier\n",
        "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = model.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\\n\", feature_importance_df)\n",
        "\n",
        "# Optional: Plot the importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='teal')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.title('Gradient Boosting Feature Importances (Breast Cancer Dataset)')\n",
        "plt.xlabel('Importance')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#Q17 Train a Gradient Boosting Regressor and evaluate using R-Squared Score\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Gradient Boosting Regressor\n",
        "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Gradient Boosting Regressor R-squared Score: {r2:.4f}\")\n",
        "\n",
        "📌 What is R² Score?\n",
        "R² = 1: Perfect predictions\n",
        "\n",
        "R² = 0: Model performs no better than the mean\n",
        "\n",
        "R² < 0: Model is worse than guessing the mean\n",
        "\n",
        "#Q18- Train an XGBoost Classifier on a dataset and compare accuracy with Gradient Boosting\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Gradient Boosting Classifier\n",
        "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gb_model.fit(X_train, y_train)\n",
        "gb_preds = gb_model.predict(X_test)\n",
        "gb_accuracy = accuracy_score(y_test, gb_preds)\n",
        "\n",
        "# XGBoost Classifier\n",
        "xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "xgb_preds = xgb_model.predict(X_test)\n",
        "xgb_accuracy = accuracy_score(y_test, xgb_preds)\n",
        "\n",
        "# Print accuracies\n",
        "print(f\"Gradient Boosting Classifier Accuracy: {gb_accuracy:.4f}\")\n",
        "print(f\"XGBoost Classifier Accuracy        : {xgb_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "#Q19 - Train a CatBoost Classifier and evaluate using F1-Score\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train CatBoost Classifier (silent mode)\n",
        "model = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=3, verbose=0, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate using F1-score\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"CatBoost Classifier F1-Score: {f1:.4f}\")\n",
        "\n",
        "#Q20- Train an XGBoost Regressor and evaluate using Mean Squared Error (MSE)\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train XGBoost Regressor\n",
        "model = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate using MSE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(f\"XGBoost Regressor Mean Squared Error (MSE): {mse:.4f}\")\n",
        "\n",
        "#Q20- Train an XGBoost Regressor and evaluate using Mean Squared Error (MSE)\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train XGBoost Regressor\n",
        "model = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate using MSE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(f\"XGBoost Regressor Mean Squared Error (MSE): {mse:.4f}\")\n",
        "\n",
        "#Q21 Train an AdaBoost Classifier and visualize feature importance\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train AdaBoost Classifier\n",
        "model = AdaBoostClassifier(n_estimators=50, learning_rate=1.0, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = model.feature_importances_\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\\n\", importance_df)\n",
        "\n",
        "# Plot feature importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(importance_df['Feature'], importance_df['Importance'], color='salmon')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.xlabel(\"Importance Score\")\n",
        "plt.title(\"AdaBoost Classifier Feature Importances (Breast Cancer Dataset)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#Q22- Train a Gradient Boosting Regressor and plot learning curves\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize model\n",
        "model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Train model stage-wise and record training and validation errors\n",
        "train_errors = []\n",
        "val_errors = []\n",
        "\n",
        "for y_pred_train in model.staged_predict(X_train):\n",
        "    train_errors.append(mean_squared_error(y_train, y_pred_train))\n",
        "\n",
        "for y_pred_val in model.staged_predict(X_val):\n",
        "    val_errors.append(mean_squared_error(y_val, y_pred_val))\n",
        "\n",
        "# Plot learning curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_errors, label='Training MSE')\n",
        "plt.plot(val_errors, label='Validation MSE')\n",
        "plt.xlabel('Number of Trees')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.title('Learning Curves for Gradient Boosting Regressor')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "#Q23 - Train an XGBoost Classifier and visualize feature importance\n",
        "from xgboost import XGBClassifier, plot_importance\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train XGBoost Classifier\n",
        "model = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(10, 8))\n",
        "plot_importance(model, max_num_features=15, importance_type='gain', title='XGBoost Feature Importance (Gain)')\n",
        "plt.show()\n",
        "\n",
        "#Q24 Train a CatBoost Classifier and plot the confusion matrix\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train CatBoost Classifier\n",
        "model = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=3, verbose=0, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred, labels=model.classes_)\n",
        "\n",
        "# Plot confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title('CatBoost Classifier Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#Q25- Train an AdaBoost Classifier with different numbers of estimators and compare accuracy\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Different values for n_estimators to test\n",
        "estimators_list = [10, 50, 100, 150, 200]\n",
        "\n",
        "accuracies = []\n",
        "\n",
        "for n in estimators_list:\n",
        "    model = AdaBoostClassifier(n_estimators=n, learning_rate=1.0, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "    print(f\"Estimators: {n} - Accuracy: {acc:.4f}\")\n",
        "\n",
        "# Plotting the results\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(estimators_list, accuracies, marker='o', color='navy')\n",
        "plt.title('AdaBoost Accuracy vs Number of Estimators')\n",
        "plt.xlabel('Number of Estimators')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "#Q26- Train a Gradient Boosting Classifier and visualize the ROC curve\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Gradient Boosting Classifier\n",
        "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute ROC curve and ROC area\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Gradient Boosting Classifier')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "#Q27- Train an XGBoost Regressor and tune the learning rate using GridSearchCV\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize model (other params fixed, tune learning_rate)\n",
        "xgb = XGBRegressor(n_estimators=100, max_depth=3, random_state=42)\n",
        "\n",
        "# Define parameter grid for learning_rate\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]\n",
        "}\n",
        "\n",
        "# Setup GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb,\n",
        "    param_grid=param_grid,\n",
        "    scoring='neg_mean_squared_error',  # minimizing MSE\n",
        "    cv=5,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Run grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameter and score\n",
        "print(f\"Best learning_rate: {grid_search.best_params_['learning_rate']}\")\n",
        "print(f\"Best CV MSE: {-grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate on test set using the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Test Set Mean Squared Error: {test_mse:.4f}\")\n",
        "\n",
        "#Q28- Train a CatBoost Classifier on an imbalanced dataset and compare performance with class weighting\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "# Create imbalanced dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=5000,\n",
        "    n_features=20,\n",
        "    n_informative=2,\n",
        "    n_redundant=10,\n",
        "    n_clusters_per_class=1,\n",
        "    weights=[0.9, 0.1],  # 90% of class 0, 10% of class 1\n",
        "    flip_y=0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Compute class weights (inversely proportional to class frequencies)\n",
        "from collections import Counter\n",
        "counter = Counter(y_train)\n",
        "total = sum(counter.values())\n",
        "class_weights = {cls: total/count for cls, count in counter.items()}\n",
        "\n",
        "print(\"Class weights:\", class_weights)\n",
        "\n",
        "# Model without class weights\n",
        "model_no_weights = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=4, verbose=0, random_state=42)\n",
        "model_no_weights.fit(X_train, y_train)\n",
        "y_pred_no_weights = model_no_weights.predict(X_test)\n",
        "f1_no_weights = f1_score(y_test, y_pred_no_weights)\n",
        "\n",
        "# Model with class weights\n",
        "model_with_weights = CatBoostClassifier(\n",
        "    iterations=100,\n",
        "    learning_rate=0.1,\n",
        "    depth=4,\n",
        "    class_weights=[class_weights[0], class_weights[1]],\n",
        "    verbose=0,\n",
        "    random_state=42\n",
        ")\n",
        "model_with_weights.fit(X_train, y_train)\n",
        "y_pred_with_weights = model_with_weights.predict(X_test)\n",
        "f1_with_weights = f1_score(y_test, y_pred_with_weights)\n",
        "\n",
        "# Print results\n",
        "print(f\"F1-Score without class weights: {f1_no_weights:.4f}\")\n",
        "print(f\"F1-Score with class weights:    {f1_with_weights:.4f}\")\n",
        "\n",
        "#Q29-   Train an AdaBoost Classifier and analyze the effect of different learning rates\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Different learning rates to test\n",
        "learning_rates = [0.01, 0.05, 0.1, 0.5, 1, 2]\n",
        "\n",
        "accuracies = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    model = AdaBoostClassifier(n_estimators=50, learning_rate=lr, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "    print(f\"Learning Rate: {lr} - Accuracy: {acc:.4f}\")\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(learning_rates, accuracies, marker='o', color='teal')\n",
        "plt.title('AdaBoost Accuracy vs Learning Rate')\n",
        "plt.xlabel('Learning Rate')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "#30- Train an XGBoost Classifier for multi-class classification and evaluate using log-loss.\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Load Iris dataset (3 classes)\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train XGBoost Classifier for multi-class classification\n",
        "model = XGBClassifier(\n",
        "    objective='multi:softprob',  # outputs probabilities for each class\n",
        "    num_class=3,                 # number of classes\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='mlogloss',\n",
        "    random_state=42\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict class probabilities on test set\n",
        "y_proba = model.predict_proba(X_test)\n",
        "\n",
        "# Calculate Log-Loss\n",
        "logloss = log_loss(y_test, y_proba)\n",
        "print(f\"Multi-class Log-Loss: {logloss:.4f}\")\n"
      ],
      "metadata": {
        "id": "ooxO5uILhtTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "don-H2DPhlOP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}